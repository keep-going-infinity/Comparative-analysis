{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4156f460-4a27-4702-83f5-f726782216b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----------------------------------------------------------\n",
    "## Dissertation Project: \n",
    "An Empirical Study on the Classification Performance of Deep Learning vs. Gradient Boosting \n",
    "on Heterogeneous Tabular Data\n",
    "\n",
    "Author: Adam Mabrouk\n",
    "\n",
    "Supervisor: Ben Ralph\n",
    "\n",
    "Institution: University of Bath\n",
    "\n",
    "created on: 01/01/2024\n",
    "\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df2b75-03cf-4ba5-9b96-8043c42713e8",
   "metadata": {},
   "source": [
    "## Due to size limitations the Datasets currently not in these folders are:\n",
    "\n",
    "1. Data pipeline folder.\n",
    "- The Tabular Data loader will not run with the out the engineered csv files.\n",
    "\n",
    "2. Visualisation\n",
    "- The visualisations to show class imbalance can bot be viewed without the raw datasets. \n",
    "\n",
    "#### The folders presented in the README list below are based on the original structure in the link mentioned below.\n",
    "#### Please use this link: https://drive.google.com/drive/folders/1BBM3cF6YhyN1BKxOPfYVsnbIkohL5GRL?usp=drive_link to access the complete code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1312cfb4-39e1-4894-8ed5-8ce0205579b9",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "- **To begin using the notebooks** \n",
    "  - **Please run the 3 notebooks to assess the raw results which form the projects main hypothesis. The results are from the classifiers, NODE, TabNet and XGBoost. NOTE: FFNN is not part of the studies main hypothesis and only used for exploratory work. 'credit_default_model_data' is not used in this study just for testing purposes.**\n",
    "  \n",
    "  - `1. Run_code_classification_ablation_results`: Will provide the classification results.\n",
    "  - `2. Run_code_model_log_loss`: Will provide the log loss results from the overfitting experiment.\n",
    "  - `3. Run_code_time_results`: Will provide the time results. \n",
    "  \n",
    "  - **Please note preliminary ablation work is exploratory and used for further investigation, it is not part of the studies main hypothesis.** \n",
    "  \n",
    "- **To run the models, please access**\n",
    "  -`Models_and_data`**: Here you can run all the models including FFNN. Instructions are provided in the code. The user can use either optuna or carry out manual tuning. The user selects the dataset and runs the model. When running the scripts, the 'time.csv' file will appear as these were manually moved when testing. an inventory of each folder is provided below. \n",
    "  \n",
    "  - `Data_pipeline`: If the user wants to make any alterations to the datasets they need to access this folder. The user can access the The ipynb notebook files labelled 01-06 should they wish to make any changes to feature engineering. An additional dataset is also included here 'credit_default_model_data' but not used in this study. The 07_Tabular_data_hetero_preprocessor is was made for the purpose of transparency within this field by creating a standardised approach when carrying out comparative analysis. The user has the option to:\n",
    "  \n",
    "  - `1. select categorical columns for either: One-hot, label encoding`\n",
    "  - `2. select split ratio.`\n",
    "  - `3. select over or undersampling`\n",
    "  - `4. Select embeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa18e8-eadc-464f-977a-5d433122982d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contents Folder \n",
    "\n",
    "Each ablation folder contains 10 csv files. Preliminary ablation folders contains only 3 csv files\n",
    "\n",
    "- **Ablation** Carried out to understand the models function\n",
    "  - **NODE Model Ablation Studies** (See Chapters 5 and 6 for details)\n",
    "      - `ablation_node`: Contains the results of NODE model ablation experiments.\n",
    "      - `entmax`: Ablation results for the entmax activation function.\n",
    "      - `gumbel_softmax`: Ablation results for the Gumbel Softmax technique.\n",
    "      - `low_depth`: Ablation results for experiments with reduced depth.\n",
    "      - `softmax`: Ablation results for the softmax activation function.\n",
    "      - `sparsemax`: Ablation results for the sparsemax activation function.\n",
    "      - `tree_increase`: Ablation results for experiments with increased tree complexity.\n",
    "  - **TabNet Model Ablation Studies** (Refer to Chapters 5 and 6)\n",
    "      - `ablation_tabnet`: Contains the results of TabNet model ablation experiments.\n",
    "      - `glu`: Ablation results for the Gated Linear Unit (GLU) activation.\n",
    "      - `mish`: Ablation results for the Mish activation function.\n",
    "      - `relaxation_factor`: Ablation results for experiments with varied relaxation factors.\n",
    "      - `relu`: Ablation results for the ReLU activation function.\n",
    "      - `sparse_loss_strength`: Ablation results for experiments with different strengths of sparse loss.\n",
    "  - **Preliminary Ablation Studies for TabNet** (Discussed in Chapter 6 and Appendix C)\n",
    "      - `preliminary_ablation_tabnet`: Contains preliminary ablation study results for the TabNet model.\n",
    "      - `Batch_size`: Ablation results for experiments with different batch sizes.\n",
    "      - `feature_dimension`: Ablation results for experiments with varied feature dimensions.\n",
    "      - `high_Lambda`: Ablation results for experiments with high Lambda regularization strength.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27bb1c4-9cd4-4a4c-9e4f-4f27ae83b4d2",
   "metadata": {},
   "source": [
    "- **Data Pipeline**\n",
    "  - **Cleaned Data**\n",
    "    - `cleaned data`: cleaned (basic pre-processing) heloc and lending club data csv files.\n",
    "    - `credit_default_model_data`: This data is not used in this study but optional for further research. This \n",
    "    folder contains the test, train and validation dataset for X and y. \n",
    "    - `feature_engineered_model_data`: This folder contains the datasets Heloc, Credit Default, Lending Club \n",
    "    and Adult Income(also known as income evaluation) after feature engineering.\n",
    "    - `heloc_model_data`: The Heloc dataset is ready to be fed into the model, hence the name model and \n",
    "    contains the test, train and validation dataset for X and y. \n",
    "    - `income_evaluation_model_data`: The Income Evaluation datasets (also known as Adult Income (AI)) is \n",
    "    ready to be fed into the model, hence the name model and contains the test, train and validation dataset \n",
    "    for X and y.\n",
    "    - `lending_club_model_data`: The Lending Club (LC) datasets is ready to be fed into the model, hence the \n",
    "    name model and contains the test, train and validation dataset for X and y.\n",
    "    - `raw_datasets`: This folder contains the raw datasets for Heloc, Credit Default, Lending Club and Adult \n",
    "    Income before any pre-processing. \n",
    "  - **Data Pipeline Notebooks** \n",
    "    - `01_lending_club_cleaner`: This notebook cleans the LC dataset.\n",
    "    - `02_lending_club_feature_engineering`: This notebook applies feature engineering methods to the LC \n",
    "    dataset.\n",
    "    - `03_heloc_data_cleaner`: This notebook cleans the Heloc dataset.\n",
    "    - `04_heloc_feature_engineering`: This notebook applies feature engineering to the Heloc dataset.\n",
    "    - `05_default_of_credit_cards_feature_engineered`: This notebook applies feature engineering to the credit \n",
    "    card default dataset which is NOT used in this study, only for additional reasearch.\n",
    "    - `06_income_evaluation_feature_engineering`: This notebook applies feature engineering to the income \n",
    "    evaluation (Adult Income) dataset. \n",
    "    - `07_Tabular_data_hetero_preprocessor`: This notebook is part of a novel design strategy that operates \n",
    "    with `Tabular_loader_class`: Datasets can be added for further research.\n",
    "    - `Tabular_loader_class`: This python script contains the pre-processing steps before the data is fed into \n",
    "    the models. Pre-processing steps are optional. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1dd678-2266-42e5-9133-11b160480a78",
   "metadata": {},
   "source": [
    "- **Log_Loss_and_Validation_AUPRC_Results**\n",
    "\n",
    "Each folder contains the results from the overfitting experiment using log loss, and auprc and contains 15 csv files. \n",
    "\n",
    "  - **node**: \n",
    "    - `node_adult_income`\n",
    "    - `node_heloc`\n",
    "    - `node_lending club`\n",
    "  - **tabnet**: \n",
    "    - `tabnet_adult_income`\n",
    "    - `tabnet_heloc`\n",
    "    - `tabnet_lending_club`\n",
    "  - **xgboost**: \n",
    "    - `xgboost_adult_income`\n",
    "    - `xgboost_heloc`\n",
    "    - `xgboost_lending_club`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb123f-088f-4370-b75b-576f1fc5729f",
   "metadata": {},
   "source": [
    "- **Model_results**\n",
    "\n",
    "Each folder contains the results from the classification experiment, 15 csv files are found in each folder.\n",
    "\n",
    "  - **node_results**: \n",
    "    - `node_adult_income`\n",
    "    - `node_heloc`\n",
    "    - `node_lending club`\n",
    "  - **tabnet_results**: \n",
    "    - `tabnet_adult_income`\n",
    "    - `tabnet_heloc`\n",
    "    - `tabnet_lending_club`\n",
    "  - **xgboost_results**: \n",
    "    - `xgboost_adult_income`\n",
    "    - `xgboost_heloc`\n",
    "    - `xgboost_lending_club`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308eba09-d189-4c2d-b82b-2caf6766da03",
   "metadata": {},
   "source": [
    "- **Model_results_for_further_testing**\n",
    "\n",
    "These results are not presented in the study and are for continued work for researchers who want to test the classifiers and/or expand on the experiments presented. \n",
    "\n",
    "  - **node**: csv files from the classification results\n",
    "    - `node_adult_income`\n",
    "    - `node_heloc`\n",
    "    - `node_lending club`\n",
    "  - **tabnet**: csv files from the classification results\n",
    "    - `tabnet_adult_income`\n",
    "    - `tabnet_heloc`\n",
    "    - `tabnet_lending_club`\n",
    "  - **xgboost**: csv files from the classification results\n",
    "    - `xgboost_adult_income`\n",
    "    - `xgboost_heloc`\n",
    "    - `xgboost_lending_club`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0abb8c9-592b-4d5d-9da7-2912e8a004ec",
   "metadata": {},
   "source": [
    "- **Models_and_data**\n",
    "\n",
    "These folders present the models used in all the experiments including the FFNN model. Use one of the four notebooks presented (FFNN, NODE, TabNet, XGBoost) to run the model. A time csv file will appear after each run as these were manually moved during testing. \n",
    "  \n",
    "  - *`datasets`*: For the purpose to run the models\n",
    "  - *`tensor_logs`*: These tensor logs are from NODE and TabNet\n",
    "  - *`data_loader.py`*: Data loader uses a random shuffle, extracting 5k to load into the model\n",
    "  - *`feed_forward_network_model.py`*: Basline classifier for the exploratory phase\n",
    "  - *`feed_forward_network_model.ipynb`*: Basline classifier for the exploratory phase\n",
    "  - *`model_training.py`*: This python script is used to train all the models\n",
    "  - *`neural_oblivious_decision_ensembles.ipynb`*: NODE notebook\n",
    "  - *`node_entmax_implementation.py`*: Entmax activation function used in ablation testing for NODE\n",
    "  - *`node_model`*: Node python script which provides comments and details of the NODE algorithm. \n",
    "  - *`requirements.txt`*: Applied to all models\n",
    "  - *`Results.py`*: Results script used for all models\n",
    "  - *`tabnet_model.py`*: TabNet model python script\n",
    "  - *`TabNet.ipynb Notebook`*: TabNet Notebook\n",
    "  - *`xgboost_model.py`*: Benchmark model XGBoost python script\n",
    "  - *`xgboost.ipynb`*: Benchmark model XGBoost Notebook script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d9386-6f42-4dc7-84ee-c89fc163524a",
   "metadata": {},
   "source": [
    "- **Shap_pictures**\n",
    "\n",
    "Each folder contains 15 pictures of shap features from each model output in the classification experiment explained in chapter 4 and 5. The first 5 most important shap features were manually counted, on which after a mean was calculated for each model. Summarised in Chapter 5 for further details. \n",
    "\n",
    "  - **node_shap**: \n",
    "    - `node_shap_adult_income`\n",
    "    - `node_shap_lending_club`\n",
    "  - **tabnet_shap**: \n",
    "    - `tabnet_adult_income_shap`\n",
    "    - `Tabnet_shap_lending_club`\n",
    "  - **xgboost_shap**: \n",
    "    - `xgboost_shap_adult_income`\n",
    "    - `xgboost_shap_lending_club`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f238f09-c56e-45e1-a469-6230dbcbe163",
   "metadata": {},
   "source": [
    "- **Time_results**\n",
    "\n",
    "Each folder contains 10 csv files of the timed runs carried out on each model. \n",
    "\n",
    "  - *`time_node`*: \n",
    "  - *`time_tabnet`*: \n",
    "  - *`time_xgboost`*: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c236e-e6e4-48bd-8767-9bd019db9b2f",
   "metadata": {},
   "source": [
    "- **Visualisation**\n",
    "\n",
    "The csv files are used to output the visualisation graphs for shap and the imbalanced classes in the datasets. \n",
    "\n",
    "  - **`csv files`**: For the purpose to run the models\n",
    "  - *`shap_visual_lending_club_and_adult_income`*: notebook\n",
    "  - *`Visualisation_class_balance_heloc`*: notebook\n",
    "  - *`Visualisation_class_balance_income_eval`*: notebook\n",
    "  - *`Visualisation_class_balance_lend_club`*: notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c190da-d0d3-4fae-8352-420bc3c43117",
   "metadata": {},
   "source": [
    "- **Notebook documents**\n",
    "\n",
    "These notebooks are to be run by the user to assess/cross reference the results of all experiment. Please open the notebook and run the jupyter file. \n",
    "\n",
    "  - **`README`**: Instructions \n",
    "  - *`Run_code_classification_ablation_results`*: Run the ipynb notebook to assess results \n",
    "  - *`Run_code_model_log_loss`*: Run the ipynb notebook to assess results \n",
    "  - *`Run the ipynb notebook to assess results `*: Run the ipynb notebook to assess results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0389180-dd46-49c8-8981-642f10900604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
